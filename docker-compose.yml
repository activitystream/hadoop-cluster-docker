version: "3"

services:

  spark-ui-proxy:
    image: registry.activitystream.com:5043/spark-ui-proxy
    networks:
      - hadoop
    ports: 
     - 9999:9999
    command: hadoop-master:8080 9999 

  hadoop-master:
    image: registry.activitystream.com:5043/hadoop-master:${BUILD_REV}
    ports:
      - 50070:50070
      - 50010:50010
      - 50020:50020
      - 50075:50075
      - 50090:50090
      - 8020:8020
      - 8088:8088
      - 8080:8080
      - 8085:8085
      - 6066:6066
      - 9000:9000
    networks:
      - hadoop
    hostname: hadoop-master
    depends_on: 
      - hadoop-slave1
      - hadoop-slave2
      - hadoop-slave3
    deploy:
      placement:
        constraints: [node.labels.hadoop.role == master]
    command: ./run.sh

  hadoop-slave3:
    image: registry.activitystream.com:5043/hadoop-node:${BUILD_REV}
    networks:
      - hadoop
    hostname: hadoop-slave3
    deploy:
      placement:
        constraints: [node.labels.hadoop.role == master]

  hadoop-slave1:
    image: registry.activitystream.com:5043/hadoop-node:${BUILD_REV}
    networks:
      - hadoop
    hostname: hadoop-slave1
    deploy:
      placement:
        constraints: [node.labels.hadoop.role == node1]

  hadoop-slave2:
    image: registry.activitystream.com:5043/hadoop-node:${BUILD_REV}
    networks:
      - hadoop
    hostname: hadoop-slave2
    deploy:
      placement:
        constraints: [node.labels.hadoop.role == node2]

networks:
  hadoop:
    external: true

volumes:
  db-data: